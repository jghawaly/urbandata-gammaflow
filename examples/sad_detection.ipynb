{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spectral Anomaly Detection (SAD)\n",
        "\n",
        "This notebook demonstrates the Spectral Anomaly Detection algorithm for identifying radioactive sources in gamma-ray time series data.\n",
        "\n",
        "## Algorithm Overview\n",
        "\n",
        "The SAD detector:\n",
        "1. **Learns background subspace** using PCA on source-absent training data\n",
        "2. **Computes reconstruction error** for new spectra: SAD(x) = ||(I - UU^T)x||²\n",
        "3. **Detects anomalies** when reconstruction error exceeds threshold\n",
        "4. **Aggregates alarms** that are close in time\n",
        "\n",
        "## Key Features\n",
        "- **Unsupervised**: Only needs background data for training\n",
        "- **Multivariate**: Captures correlations between energy channels\n",
        "- **Interpretable**: Reconstruction error quantifies \"how different from normal\"\n",
        "\n",
        "## Reference\n",
        "NOTE: There is no guarantee that this is a faithful representation of the work described below!\n",
        "\n",
        "Miller, K., & Dubrawski, A. (2018). Gamma-ray source detection with small sensors. IEEE Transactions on Nuclear Science, 65(4), 1047-1058.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "Using the TopCoder Urban Data Challenge dataset (mobile NaI detector).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "sys.path.insert(0, '../../gammaflow')\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# GammaFlow imports\n",
        "from gammaflow import Spectrum, SpectralTimeSeries, ListMode\n",
        "from gammaflow.core.spectra import Spectra\n",
        "from gammaflow.visualization import plot_count_rate_time_series\n",
        "\n",
        "# Detection algorithm\n",
        "from src.detectors import SADDetector\n",
        "\n",
        "# Configure plotting\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"✅ SAD Detection - Ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data Helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_listmode_run(run_id, data_dir='../topcoder', dataset='training'):\n",
        "    \"\"\"Load a run and return ListMode object + metadata.\"\"\"\n",
        "    data_path = Path(data_dir)\n",
        "    \n",
        "    # Load listmode data\n",
        "    run_file = data_path / dataset / f\"{run_id}.csv\"\n",
        "    data = pd.read_csv(run_file, header=None, names=['time_delta_us', 'energy_keV'])\n",
        "    \n",
        "    # Convert to seconds\n",
        "    time_deltas = data['time_delta_us'].values * 1e-6\n",
        "    energies = data['energy_keV'].values\n",
        "    \n",
        "    # Load metadata\n",
        "    answer_key_file = data_path / 'scorer' / f'answerKey_{dataset}.csv'\n",
        "    answer_key = pd.read_csv(answer_key_file)\n",
        "    metadata = answer_key[answer_key['RunID'] == run_id].iloc[0].to_dict()\n",
        "    \n",
        "    # Map SourceID to source name\n",
        "    source_map = {\n",
        "        0: 'Background',\n",
        "        1: 'HEU',\n",
        "        2: 'WGPu',\n",
        "        3: 'I-131',\n",
        "        4: 'Co-60',\n",
        "        5: 'Tc-99m',\n",
        "        6: 'Tc-99m + HEU'\n",
        "    }\n",
        "    metadata['SourceName'] = source_map.get(metadata['SourceID'], f\"Unknown({metadata['SourceID']})\")\n",
        "    \n",
        "    # Create ListMode\n",
        "    listmode = ListMode(time_deltas, energies)\n",
        "    \n",
        "    return listmode, metadata\n",
        "\n",
        "print(\"✅ Load function ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Background Training Data\n",
        "\n",
        "For SAD, we need background-only data to learn the \"normal\" spectral subspace via PCA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ALL background runs for training\n",
        "# First, identify all background runs from the answer key\n",
        "data_path = Path('../topcoder')\n",
        "answer_key_file = data_path / 'scorer' / 'answerKey_training.csv'\n",
        "answer_key = pd.read_csv(answer_key_file)\n",
        "\n",
        "# Get all background runs (SourceID == 0)\n",
        "background_runs = answer_key[answer_key['SourceID'] == 0]\n",
        "background_run_ids = background_runs['RunID'].values\n",
        "\n",
        "print(f\"Found {len(background_run_ids)} background runs in the dataset\")\n",
        "print(f\"Loading all background runs for training...\")\n",
        "\n",
        "background_spectra_list = []\n",
        "total_spectra = 0\n",
        "\n",
        "for run_id in tqdm(background_run_ids):\n",
        "    try:\n",
        "        listmode, metadata = load_listmode_run(run_id)\n",
        "        \n",
        "        # Convert to time series (5-second integration for more training samples)\n",
        "        ts = SpectralTimeSeries.from_list_mode(\n",
        "            listmode,\n",
        "            integration_time=5.0,\n",
        "            stride_time=5.0,\n",
        "            energy_bins=512,\n",
        "            energy_range=(0, 3000)\n",
        "        )\n",
        "        \n",
        "        background_spectra_list.append(ts)\n",
        "        total_spectra += ts.n_spectra\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n  Warning: Failed to load run {run_id}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n✅ Successfully loaded {len(background_spectra_list)} runs\")\n",
        "print(f\"   Total time windows: {total_spectra}\")\n",
        "\n",
        "# Combine all background spectra into one training set\n",
        "all_bg_spectra = []\n",
        "for ts in background_spectra_list:\n",
        "    all_bg_spectra.extend(ts.spectra)\n",
        "\n",
        "# Create a combined training dataset\n",
        "background_training = Spectra(all_bg_spectra)\n",
        "\n",
        "print(f\"\\n✅ Training dataset: {len(background_training.spectra)} spectra from {len(background_spectra_list)} runs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train SAD Detector\n",
        "\n",
        "**Important Note on Normalization**: We do NOT normalize spectra to unit integral because:\n",
        "- Normalization removes total count rate information\n",
        "- Source detection relies on the COUNT RATE increase being anomalous\n",
        "- With normalization, a 2x increase in counts disappears, and only spectral shape matters\n",
        "- For weak sources or Compton-dominated spectra, shape changes are subtle\n",
        "- Result: The metric goes DOWN when source is present (backwards!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train detector\n",
        "detector = SADDetector(\n",
        "    n_components=5,      # Number of principal components\n",
        "    normalize=False,     # DON'T normalize - we want to preserve count rate info!\n",
        "    aggregation_gap=2.0  # Merge alarms < 2 seconds apart\n",
        ")\n",
        "\n",
        "print(\"Training SAD detector...\")\n",
        "detector.fit(background_training)\n",
        "\n",
        "# Check explained variance\n",
        "var_ratios = detector.get_explained_variance_ratio()\n",
        "cum_var = detector.get_cumulative_variance_explained()\n",
        "\n",
        "print(f\"\\n✅ Training complete!\")\n",
        "print(f\"\\nExplained variance by component:\")\n",
        "for i, var in enumerate(var_ratios, 1):\n",
        "    print(f\"  PC{i}: {var*100:.2f}%\")\n",
        "print(f\"\\nCumulative variance explained: {cum_var*100:.2f}%\")\n",
        "\n",
        "# Visualize explained variance\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "# Individual variance\n",
        "ax1.bar(range(1, len(var_ratios)+1), var_ratios*100, color='steelblue', edgecolor='black')\n",
        "ax1.set_xlabel('Principal Component', fontsize=12)\n",
        "ax1.set_ylabel('Explained Variance (%)', fontsize=12)\n",
        "ax1.set_title('Individual Explained Variance', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Cumulative variance\n",
        "ax2.plot(range(1, len(var_ratios)+1), np.cumsum(var_ratios)*100, 'o-', \n",
        "         linewidth=2, markersize=8, color='steelblue')\n",
        "ax2.axhline(95, color='red', linestyle='--', linewidth=1, alpha=0.5, label='95%')\n",
        "ax2.set_xlabel('Number of Components', fontsize=12)\n",
        "ax2.set_ylabel('Cumulative Explained Variance (%)', fontsize=12)\n",
        "ax2.set_title('Cumulative Explained Variance', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Visualizing PCA Reconstructions\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize PCA Reconstructions\n",
        "\n",
        "Let's see how well the PCA model reconstructs background spectra. Good reconstructions mean low SAD scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select 10 random background spectra to visualize\n",
        "np.random.seed(42)\n",
        "n_examples = 10\n",
        "example_indices = np.random.choice(len(background_training.spectra), n_examples, replace=False)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, idx in enumerate(example_indices):\n",
        "    spec = background_training.spectra[idx]\n",
        "    \n",
        "    # Prepare spectrum (same as detector does)\n",
        "    if detector.normalize:\n",
        "        counts = spec.counts / spec.counts.sum()\n",
        "    else:\n",
        "        counts = spec.counts\n",
        "    \n",
        "    # Get reconstruction from PCA\n",
        "    X = counts.reshape(1, -1)\n",
        "    X_reconstructed = detector.pca.inverse_transform(detector.pca.transform(X))\n",
        "    \n",
        "    # Calculate residual\n",
        "    residual = X - X_reconstructed\n",
        "    sad_score = np.sum(residual ** 2)\n",
        "    \n",
        "    # Get energy centers for plotting\n",
        "    energy_centers = spec.energy_centers\n",
        "    \n",
        "    # Plot\n",
        "    ax = axes[i]\n",
        "    ax.plot(energy_centers, counts, 'k-', linewidth=1.5, alpha=0.7, label='Original')\n",
        "    ax.plot(energy_centers, X_reconstructed[0], 'r--', linewidth=1.5, alpha=0.8, label='Reconstruction')\n",
        "    ax.fill_between(energy_centers, 0, residual[0], alpha=0.3, color='orange', label='Residual')\n",
        "    \n",
        "    ax.set_xlabel('Energy (keV)', fontsize=10)\n",
        "    ax.set_ylabel('Normalized Counts' if detector.normalize else 'Counts', fontsize=10)\n",
        "    ax.set_title(f'Spectrum {idx} | SAD Score: {sad_score:.6f}', fontsize=11, fontweight='bold')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xlim(0, 3000)\n",
        "\n",
        "plt.suptitle(f'Background Spectrum Reconstructions (using {detector.n_components} PCs)', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✅ PCA reconstruction quality visualization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Threshold Based on False Alarm Rate\n",
        "\n",
        "We'll calibrate the threshold using the **alarms per hour** metric, which is the standard for operational radiation detection systems. ANSI N42.48 typically requires **< 1 alarm/hour** for nuisance alarm rates.\n",
        "\n",
        "This is different from simply thresholding a percentage of spectra, because:\n",
        "- Alarm aggregation reduces the alarm count (consecutive high scores = 1 alarm)\n",
        "- The metric is time-based, making it more operationally meaningful\n",
        "- It accounts for temporal dynamics of the detector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set threshold based on alarms per hour (ANSI standard metric)\n",
        "# ANSI N42.48 typically requires < 1 alarm/hour\n",
        "#\n",
        "# NOTE: Using just the first background run for quick calibration and fair\n",
        "# algorithm comparison.\n",
        "#\n",
        "# ⚠️  FOR REAL-WORLD/PRODUCTION USE: You should calibrate on MULTIPLE background\n",
        "#     runs (e.g., 20+) to get robust statistics and account for environmental\n",
        "#     variations, detector drift, and different background conditions. Using a\n",
        "#     single run is acceptable for testing and algorithm comparison, but not\n",
        "#     for operational deployment.\n",
        "alarms_per_hour = 0.5  # Conservative target\n",
        "\n",
        "print(f\"Setting threshold for {alarms_per_hour} alarms per hour (ANSI compliant)...\")\n",
        "print(\"This uses iterative search to find the optimal threshold...\")\n",
        "\n",
        "# Need to convert Spectra to a time series for temporal FAR calculation\n",
        "# Reconstruct time series from the individual runs\n",
        "print(\"\\nReconstructing time series for FAR calculation...\")\n",
        "detector.set_threshold_by_far(background_spectra_list[0], alarms_per_hour=alarms_per_hour)\n",
        "\n",
        "print(f\"\\n✅ Threshold set: {detector.threshold:.6f}\")\n",
        "print(f\"   Converged to {len(detector.alarms)} alarms\")\n",
        "\n",
        "# Calculate total observation time\n",
        "total_time_hours = (background_spectra_list[0].timestamps[-1] - background_spectra_list[0].timestamps[0]) / 3600.0\n",
        "actual_far = len(detector.alarms) / total_time_hours\n",
        "print(f\"   Achieved FAR: {actual_far:.2f} alarms/hour\")\n",
        "\n",
        "# Visualize score distribution and threshold\n",
        "print(\"\\nCalculating score distribution for visualization...\")\n",
        "all_scores = detector.score_time_series(background_spectra_list[0])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram of scores\n",
        "ax1 = axes[0]\n",
        "ax1.hist(all_scores, bins=50, alpha=0.7, edgecolor='black')\n",
        "ax1.axvline(detector.threshold, color='red', linestyle='--', linewidth=2, \n",
        "            label=f'Threshold = {detector.threshold:.6f}')\n",
        "ax1.set_xlabel('SAD Score', fontsize=12)\n",
        "ax1.set_ylabel('Count', fontsize=12)\n",
        "ax1.set_title('Distribution of SAD Scores on Background Data', fontsize=13)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Cumulative distribution\n",
        "ax2 = axes[1]\n",
        "sorted_scores = np.sort(all_scores)\n",
        "cumulative = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores)\n",
        "ax2.plot(sorted_scores, cumulative, linewidth=2)\n",
        "ax2.axvline(detector.threshold, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Threshold = {detector.threshold:.6f}')\n",
        "ax2.axhline(1 - (len(all_scores[all_scores > detector.threshold]) / len(all_scores)),\n",
        "            color='green', linestyle=':', alpha=0.5,\n",
        "            label=f'False positive rate')\n",
        "ax2.set_xlabel('SAD Score', fontsize=12)\n",
        "ax2.set_ylabel('Cumulative Probability', fontsize=12)\n",
        "ax2.set_title('Cumulative Distribution Function', fontsize=13)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nScore statistics:\")\n",
        "print(f\"  Mean: {all_scores.mean():.6f}\")\n",
        "print(f\"  Std: {all_scores.std():.6f}\")\n",
        "print(f\"  Min: {all_scores.min():.6f}\")\n",
        "print(f\"  Max: {all_scores.max():.6f}\")\n",
        "print(f\"  Threshold percentile: {(all_scores < detector.threshold).sum() / len(all_scores) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on Run with Source\n",
        "\n",
        "Now let's test the detector on a run with a radioactive source.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load I-131 run with strongest source\n",
        "# Find I-131 runs and sort by Speed/Offset (higher = stronger)\n",
        "data_path = Path('../topcoder')\n",
        "answer_key = pd.read_csv(data_path / 'scorer' / 'answerKey_training.csv')\n",
        "i131_runs = answer_key[answer_key['SourceID'] == 3].copy()\n",
        "i131_runs = i131_runs.sort_values('Speed/Offset', ascending=False)\n",
        "\n",
        "# Pick the strongest run\n",
        "test_run_id = i131_runs.iloc[0]['RunID']\n",
        "speed_offset = i131_runs.iloc[0]['Speed/Offset']\n",
        "\n",
        "print(f\"Loading I-131 test run {test_run_id}...\")\n",
        "print(f\"  Speed/Offset: {speed_offset:.2f} (stronger is better)\")\n",
        "listmode, metadata = load_listmode_run(int(test_run_id))\n",
        "\n",
        "print(f\"\\nRun Metadata:\")\n",
        "print(f\"  Source: {metadata['SourceName']}\")\n",
        "print(f\"  Source Time: {metadata['SourceTime']:.1f} seconds\")\n",
        "print(f\"  Speed/Offset: {metadata['Speed/Offset']:.2f}\")\n",
        "print(f\"\\n{listmode}\")\n",
        "\n",
        "# Convert to time series with 1-second integration\n",
        "print(\"\\nConverting to SpectralTimeSeries...\")\n",
        "test_time_series = SpectralTimeSeries.from_list_mode(\n",
        "    listmode,\n",
        "    integration_time=1.0,\n",
        "    stride_time=1.0,\n",
        "    energy_bins=512,\n",
        "    energy_range=(0, 3000)\n",
        ")\n",
        "\n",
        "print(f\"\\nCreated: {test_time_series}\")\n",
        "print(f\"  Number of spectra: {test_time_series.n_spectra}\")\n",
        "print(f\"  Time coverage: {test_time_series.timestamps[0]:.1f} to {test_time_series.timestamps[-1]:.1f} s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run SAD Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Running SAD Detection\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"  n_components: {detector.n_components}\")\n",
        "print(f\"  Threshold: {detector.threshold:.6f}\")\n",
        "print(f\"  Normalize: {detector.normalize}\")\n",
        "print()\n",
        "\n",
        "# Process time series\n",
        "sad_scores = detector.process_time_series(test_time_series)\n",
        "\n",
        "# Get summary\n",
        "summary = detector.get_alarm_summary()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Detection Results\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Number of alarms: {summary['n_alarms']}\")\n",
        "print(f\"Total alarm time: {summary['total_alarm_time']:.2f} seconds\")\n",
        "if summary['n_alarms'] > 0:\n",
        "    print(f\"Mean alarm duration: {summary['mean_duration']:.2f} seconds\")\n",
        "    print(f\"Peak SAD score: {summary['max_peak_metric']:.6f}\")\n",
        "\n",
        "if detector.alarms:\n",
        "    print(f\"\\nAlarm Events:\")\n",
        "    true_source_time = metadata['SourceTime']\n",
        "    for i, alarm in enumerate(detector.alarms, 1):\n",
        "        print(f\"  {i}. {alarm}\")\n",
        "        # Compare to ground truth\n",
        "        if alarm.start_time <= true_source_time <= alarm.end_time:\n",
        "            print(f\"      ✅ Captured true source (t={true_source_time:.1f}s)\")\n",
        "        else:\n",
        "            time_diff = min(abs(alarm.start_time - true_source_time),\n",
        "                          abs(alarm.end_time - true_source_time))\n",
        "            print(f\"      ⚠️  Offset from true source: {time_diff:.1f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot SAD scores with alarm overlay\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
        "\n",
        "# Get times and count rates\n",
        "times = test_time_series.timestamps\n",
        "count_rates = np.array([\n",
        "    float(s.counts.sum()) / float(s.live_time if s.live_time is not None else s.real_time)\n",
        "    for s in test_time_series.spectra\n",
        "])\n",
        "\n",
        "# Plot 1: Count rate\n",
        "ax1.step(times, count_rates, where='post', color='black', linewidth=1.5, label='Count rate')\n",
        "ax1.set_ylabel(r'Count Rate (s$^{-1}$)', fontsize=12)\n",
        "ax1.set_title(f'SAD Detection Results - Run {test_run_id} ({metadata[\"SourceName\"]})', \n",
        "              fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Overlay alarm regions\n",
        "for i, alarm in enumerate(detector.alarms):\n",
        "    ax1.axvspan(alarm.start_time, alarm.end_time, alpha=0.3, color='red', \n",
        "                label='Alarm' if i == 0 else '')\n",
        "    # Mark peak\n",
        "    peak_idx = np.argmin(np.abs(times - alarm.peak_time))\n",
        "    ax1.plot(alarm.peak_time, count_rates[peak_idx],\n",
        "            'r*', markersize=15, label='Peak' if i == 0 else '')\n",
        "\n",
        "# Mark true source time\n",
        "if metadata['SourceID'] != 0:\n",
        "    ax1.axvline(metadata['SourceTime'], color='green', linestyle='--', \n",
        "                linewidth=2, label='True source')\n",
        "\n",
        "ax1.legend(loc='best')\n",
        "\n",
        "# Plot 2: SAD scores\n",
        "ax2.step(times, sad_scores, where='post', color='blue', linewidth=1.5, label='SAD score')\n",
        "ax2.axhline(detector.threshold, color='red', linestyle='--', linewidth=2, \n",
        "            label=f'Threshold ({alarms_per_hour} alarms/hr)')\n",
        "ax2.set_xlabel('Time (seconds)', fontsize=12)\n",
        "ax2.set_ylabel('SAD Score (Reconstruction Error)', fontsize=12)\n",
        "ax2.set_title('Spectral Anomaly Metric', fontsize=12)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend(loc='best')\n",
        "\n",
        "# Use log scale for better visibility\n",
        "ax2.set_yscale('log')\n",
        "\n",
        "# Overlay alarm regions on metric plot too\n",
        "for alarm in detector.alarms:\n",
        "    ax2.axvspan(alarm.start_time, alarm.end_time, alpha=0.2, color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Visualization complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
